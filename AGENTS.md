# Gemini Code Exploration: `apantli`

This document provides a comprehensive overview of the `apantli` project, generated by Gemini after analyzing the codebase.

## Project Overview

`apantli` is a lightweight, local-first LLM (Large Language Model) proxy server written in Python. Its primary purpose is to provide a unified, OpenAI-compatible API endpoint that can route requests to various LLM providers (like OpenAI, Anthropic, Gemini, etc.) while tracking costs in a local SQLite database.

It is designed as a developer tool for local use, enabling easy switching between models, tracking costs, and comparing model performance without requiring heavy infrastructure like Docker or a dedicated cloud database.

The key features are:

*   **OpenAI-Compatible API:** Acts as a drop-in replacement for any tool that integrates with the OpenAI API.
*   **Multi-Provider Routing:** Uses the `LiteLLM` library to translate requests and route them to the appropriate backend provider.
*   **Cost Tracking:** All requests and their calculated costs are logged to a local SQLite database (`requests.db`).
*   **Web Dashboard:** A built-in web interface (served on port 4000 by default) provides real-time statistics, request history with advanced filtering, and model performance metrics.
*   **Interactive Playground:** A side-by-side model comparison interface allows for simultaneous prompting of up to three different models, with real-time streaming and independent parameter controls.
*   **Zero Cloud Dependencies:** The entire application runs locally, using a file-based SQLite database for storage.

## Core Architecture

**Request Flow**: Client → FastAPI (`server.py`) → Config lookup → API key resolution → LiteLLM SDK → Provider → Response + cost calc → Async DB log → Client

**Module Structure** (~1,900 lines core + ~5,000 lines UI):
- `apantli/server.py` (887 lines) - FastAPI app, HTTP routes, request orchestration
- `apantli/config.py` (189 lines) - Configuration with Pydantic validation
- `apantli/database.py` (506 lines) - Async database operations with aiosqlite
- `apantli/llm.py` (27 lines) - Provider inference
- `apantli/errors.py` (129 lines) - Error formatting
- `apantli/utils.py` (117 lines) - Timezone utilities

**Key Files**:
- `config.yaml` - Model definitions, API key refs
- `.env` - API keys (gitignored)
- `requests.db` - SQLite (full request/response logs + costs)
- `templates/dashboard.html` (502 lines) - Dashboard UI structure
- `templates/compare.html` (258 lines) - Playground UI structure
- `apantli/static/css/dashboard.css` (1,451 lines) - Dashboard styles
- `apantli/static/css/compare.css` (482 lines) - Playground styles
- `apantli/static/js/dashboard.js` (1,728 lines) - Dashboard logic
- `apantli/static/js/compare.js` (556 lines) - Playground logic
- `tests/` - Unit and integration tests (69 test cases)


## Building and Running

The project uses `uv` for managing dependencies and running scripts within a virtual environment.

### Initial Setup

1.  **Install Dependencies:**
    ```bash
    uv sync
    ```

2.  **Activate Virtual Environment:**
    ```bash
    source .venv/bin/activate
    ```

3.  **Configure Environment:**
    *   Copy the example `.env` file: `cp .env.example .env`
    *   Add your API keys for the desired LLM providers to the `.env` file.
    *   Review and customize the model list in `config.yaml`.

### Running the Server

*   **Start the server:**
    ```bash
    apantli
    ```
    The server will be accessible at `http://localhost:4000`.

*   **Start in development mode (with auto-reload):**
    ```bash
    apantli --reload
    ```

## Implementation Details

**Application State**:
- Config and Database instances stored in FastAPI's `app.state` for dependency injection.
- `app.state.config`: Config instance with all model configurations.
- `app.state.db`: Database instance for async database operations.
- `app.state.model_map`: Pre-computed dict of model parameters for fast lookups.
- Benefits: Clean dependency injection, testable, no hidden global state.

**Configuration (`config.py`)**:
- Pydantic models: `ModelConfig` (per-model settings), `Config` (overall configuration).
- Validation: API key format (`os.environ/VAR_NAME`), environment variable existence warnings.
- Type-safe: Strong typing with Pydantic for early error detection.
- Reload support: Can reload config without a server restart.
- Parameter precedence: Config provides defaults; client values (except null) override config.

**Database (`database.py`)**:
- Async operations: `aiosqlite` for non-blocking I/O.
- `Database` class: Encapsulates all database operations with async methods.
- Connection management: Context managers (`_get_connection()`).
- Core methods: `init()`, `log_request()`, `get_requests()`, `get_stats()`, `get_daily_stats()`, `get_hourly_stats()`.
- Cost calculation: Uses `litellm.completion_cost()` during `log_request()`.
- Statistics queries: Encapsulated in `Database` class methods for clarity.
- Performance: Non-blocking async operations, typically ~1-5ms per operation.

**LLM Integration (`llm.py`)**:
- Provider inference: Pattern matching for `gpt-*`/`o1-*` → openai, `claude*` → anthropic, etc.
- Single SDK: `LiteLLM` for multi-provider routing, automatic cost calculation, and OpenAI format normalization.
- Streaming support: Full Server-Sent Events (SSE) implementation.

**Dashboard & Playground**:
- **Dashboard**: A Jinja2 template at `/` using Alpine.js for reactivity. It features 4 tabs (Stats, Calendar, Models, Requests) and auto-refreshes the Stats tab every 5 seconds. A navigation link to the Playground is in the header.
- **Playground**: A side-by-side model comparison UI at `/compare`. It's a frontend-only implementation with 3 independent slots for model comparison, parallel streaming to the proxy endpoint, and localStorage persistence for conversations. See `PLAYGROUND.md` for details.

**Error Handling (`errors.py`)**:
- Comprehensive implementation with configurable timeouts/retries. See `ERROR_HANDLING.md`.
- OpenAI-compatible format: `{"error": {"message", "type", "code"}}`.
- Status code mapping: 401 (auth), 404 (not found), 429 (rate limit), 502 (connection), 503 (provider error), 504 (timeout), 500 (other).
- Streaming errors: Sent in SSE format `data: {"error": {...}}` followed by `data: [DONE]`.

## API Endpoints

All routes defined in `apantli/server.py`. See `API.md` for the full reference.

*   **Primary**: `/v1/chat/completions`, `/chat/completions` (POST) - OpenAI-compatible proxy with streaming support.
*   **Health**: `/health` (GET) - Returns `{"status": "ok"}`.
*   **Stats**: `/stats` (GET, includes performance metrics), `/stats/daily`, `/stats/date-range`.
*   **Data**: `/models` (GET, includes default parameters), `/requests` (GET), `/errors` (DELETE).
*   **UI**: `/` (GET) - Dashboard, `/compare` (GET) - Playground, `/static/*` - Frontend assets.

## Key Code Patterns

**Import Pattern (`server.py`)**:
```python
from apantli.config import DEFAULT_TIMEOUT, DEFAULT_RETRIES, Config
from apantli.database import Database, RequestFilter
from apantli.errors import build_error_response
from apantli.llm import infer_provider_from_model
from apantli.utils import convert_local_date_to_utc_range
```

**Config Usage**:
```python
# Initialize config
config = Config("config.yaml")

# Get specific model configuration
model_config = config.get_model("gpt-4")
if model_config:
    litellm_params = model_config.to_litellm_params()
    api_key = model_config.get_api_key()

# Get all models as a dict (for caching in app.state)
model_map = config.get_model_map({
    'timeout': 120,
    'num_retries': 3
})
```

**Database Usage**:
```python
# Initialize database
db = Database("requests.db")
await db.init()

# Log a request
await db.log_request(model, provider, response, duration_ms, request_data)

# Query with filters
filters = RequestFilter(
    offset=0,
    limit=50,
    provider="openai",
    model="gpt-4"
)
results = await db.get_requests(filters)

# Get statistics
stats = await db.get_stats(time_filter="")
```

## Testing

The project has a comprehensive test suite with 69 test cases and uses `mypy` for static type checking.

*   **Unit tests**: `tests/test_config.py`, `test_database.py`, `test_llm.py`, `test_errors.py`, `test_utils.py`.
*   **Integration tests**: `tests/integration/test_proxy.py`, `test_error_handling.py`.
*   **Type checking**: `mypy apantli/`.

### Running Tests and Checks

The project includes a `Makefile` with convenient targets for quality checks.

*   **Run all checks (type checking and tests):**
    ```bash
    make all
    ```
*   **Run unit tests:**
    ```bash
    make test
    ```
*   **Run static type checking with MyPy:**
    ```bash
    make typecheck
    ```

## Security

*   API keys should be stored in a `.env` file (which is gitignored).
*   The database file (`requests.db`) contains full conversation history and may contain API keys; ensure its file permissions are properly restricted.
*   The dashboard is unauthenticated and intended for local use only.
*   By default, the server binds to `0.0.0.0`. Use the `--host 127.0.0.1` argument to restrict access to the local machine.